*data_collection.py file is used for collecting the facial expressions and The main function of the script is to capture and process real-time video input from a webcam, extracting facial and hand landmarks using Mediapipe, and saving the collected landmark data into a NumPy array file for further analysis or use.
*data_training.py - The main function of the script is to load landmark data from multiple NumPy files, preprocess it, convert the labels to one-hot encoding, shuffle the data, and then train a neural network model using Keras to classify the data based on the extracted landmarks. The trained model and the labels are saved for future use.
*inference.py - The main function of the script is to capture real-time video, text, and audio inputs, process them to extract relevant features, and predict the corresponding emotions using pre-trained models for facial, text, and audio emotion recognition.
*data Set used for the audio emotion detection- https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess
*Training model of audio-The main function of the script is to load an audio dataset, extract MFCC features from each audio file, encode the emotion labels, split the data into training and testing sets, build and train a neural network model for audio emotion recognition, evaluate the model, and save the trained model and label encodings for future use.
